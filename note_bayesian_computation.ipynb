{
 "metadata": {
  "name": "",
  "signature": "sha256:7f2722756bb694269d96750897653b313f1fe0b2aaa618210e68057e3d4a652c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bayesian Computation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Key theoretical difference\n",
      "* Bayesian seems more natural: quantifying the unknown conditional on the data\n",
      "* Key basic concepts: posterior, prior, conjugate, likelihood\n",
      "* Once we know basic concepts, we can use computation instead of math\n",
      "* Bayesian is flexible on procedure"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bayesian Method\n",
      "* Model all the observables and unobservables\n",
      "* Everything can be modeled\n",
      "* Validate and check the model\n",
      "* Process is very iterative\n",
      "* Full probability modeling is a better term\n",
      "* Bayesian is just a model and approach\n",
      "* Conjugate prior: family of distribution for posterior is the same as prior\n",
      "* Start with certain family, e.g beta distribution. You observe data, get your posterior, as still in the same family. If that's the case, it's conjugate\n",
      "* Binomial, Normal, Beta, Poisson\n",
      "\n",
      "#Bayesian Methods for Hackers\n",
      "* Why not just use pseudocount? \n",
      "* Benefit of Bayesian: \n",
      "    * Measure of confidence, create an interval. Bayesian can make a probability statement about p. What's the mean? median? etc See your uncertainty.\n",
      "    * Flexibility: Add complexity, or have prior information about the data or users, we can incorporate into the model. Build increasing sophisticated model.\n",
      "* Do not need to rank according to Posterior distribution mean. We can choose a lower limit to be conservative\n",
      "* You can use posterior mean, mode or quantile\n",
      "* Bayesian Bandits: example, many different promotions or advertising and comparing them. It just like many slot machines.\n",
      "* Each slot machine has their own prize but do not know which one give you higher rewards\n",
      "* Exploration-Exploitation - how much time you spend trying new promotions or milking one which is great\n",
      "* Bayesian Algorithm (simple - for bayesian bandits)\n",
      "    * Sample from the prior of each bandit\n",
      "    * Select the bandit with largest sampled value\n",
      "    * update the prior with that bandit (**posterior becomes new prior**)\n",
      "    * repeat\n",
      "* Getting more and more data, prior will improve\n",
      "* Using computation can be better than nicest theorem\n",
      "\n",
      "#Lasso and Sparsity\n",
      "* In order to do least squares, you need n > p (classical)\n",
      "* Today, there are more features than data points\n",
      "* Hot now: Lasso - adding penalty to sum of squares\n",
      "* Useful if you believe a few features explain most of the pattern\n",
      "* Lasso has a Bayesian Interpretation: posterior mode, with independent Laplace distribution priors on the parameters\n",
      "* Useful technique to cut down dimensionality\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Kidney Cancer\n",
      "* Law of large numbers: if sample size large, the proportion of sample which is true will be close to the truth\n",
      "* Simple Algorithm:\n",
      "    * Poisson distribution: dealing with rare events\n",
      "    * y ~ Pois(10 $\\theta$): $\\theta$ is the true probability of getting kidney cancer\n",
      "    * $\\theta$ ~ Gamma($\\alpha$, $\\lambda$)\n",
      "    * Weighted combination of data and prior mean: w X data mean + (1-w) prior mean\n",
      "    * Weight depends on the sample size. Small sample size, put much more weight on prior mean (as prior makes a huge difference in the prediction)\n",
      "        * if only 1 person vote, the prior will be very important. So more weight to increase it's importance\n",
      "* Generate a lot from reading the posterior distribution\n",
      "\n",
      "#Markov Chain\n",
      "* Markov Chain: Random walk from state to state, do not have to keep track of past history. Just concern with current state.\n",
      "* MCMC: design a Markov Chain and run it.\n",
      "* Making Markovian assumption, you build and design your Markov chain. So it will converge once you run the Monte Carlo simulation.\n",
      "* How to come up with Markov chain?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}